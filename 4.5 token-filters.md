* Standard Token Filter (standard). Doesn't do anything. Acts as a placeholder for future versions.
   `[I'm, in, the, mood, for, drinking, semi, dry, red, wine] -> [I'm, in, the, mood, for, drinking, semi, dry, red, wine]`
* Lowercase Token Filter (lowercase). Normalizes terms to lowercase
   `[I'm, in, the, mood, for, drinking, semi, dry, red, wine] -> [i'm, in, the, mood, for, drinking, semi, dry, red, wine]`
* Uppercase Token Filter (uppercase). Normalizes terms to uppercase
   `[I'm, in, the, mood, for, drinking, semi, dry, red, wine] -> [I'M, IN, THE, MOOD, FOR, DRINKING, SEMI, DRY, RED, WINE]`
* NGram Token Filter (nGram). Emits N-grams of the specified length based on the provided terms.
    `[Red, wine] -> [R, Re, e, ed, d, w, wi, i, in, n, ne, e]`
    Isn't it the same as a tokenizer? Yes. The reason why there is a token for doing the same thing, is that you can use the filter with a different tokenizer then the n-gram tokenizer. So if you want to change how text input is tokenized before generating ngrams, then you can do it by building a custom analyzer with a tokenizer of your choice and then the n-gram token filter.
* Edge Ngram Token Filter (edgeNGram). Emits N-grams of each term beginning from the start of the term.
* Stop Token Filter (stop). Removes stop words.
  `[I'm, in, the, mood, for, drinking, semi, dry, red, wine] -> [I'm, mood, drinking, semi, dry, red, wine]`
  ES provides a list of stop words for most languages that can be used. But you have an option of specifying your own configuration if you need to. 
* Word Delimiter Token Filter (word_delimiter). Splits words into subwords and performs transformations on subword groupds/
  `[Wi-Fi, PowerShell, CE1000, Andy's] -> [Wi, Fi, Power, Shell, CE, 1000, Andy]`
  It splits base on a number of rules. It splits into terms at every non-alphanumeric characters. 
* Stemmer Token Filter (stemmer). Stems words for the specified language.
  `[I'm, in, the, mood, for, drinking, semi, dry, red, wine] -> [I'm, in, the, mood, for, drink, semi, dry, red, wine]`
  It just reduces words to their base form. If you want some words from being stemmed, you can use a Keyword Marker Token Filter
* Keyword Marker Token Filter (keyword_marker). Protects words from being modified by stemmers.
  Protected terms: [drinking]
* Snowball Token Filter (snowball). Stems words based on a Snowball algorithm. This requires to specify a language you do stemming for.
* Synonym Token Filter (synonym). Adds or replaces tokens based on a synonym configuration.
  `[I, am, very, happy] -> [I, am, very, happy/delighted]`
  If we want to define that a word 'delighted' is a synonym to 'happy', and therefore I don't care which of the words I use in search. It can be accomplished by supplying a configuration file with words that have the same meaning. 

  
