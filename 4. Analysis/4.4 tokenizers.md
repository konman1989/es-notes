## Overview

Tokenaziers are grouped into 3 main categories:
 
* Word oriented tokenizers. Typically used for tokenizing full text into individual words. They work at a word level and the output will be easily readable for humans. 
   - Standard Tokenizer. Divides text into terms on word boundaries and removes most symbols. Usually the best choice. It basically splits text into terms based on white spaces and certain symbols. 
   - Letter Tokenizer. Divides text into terms when encountering a character that is not a letter. It splits into token whenever it encounters a character that is not a letter.
   - Lowercase Tokeinzer. It works exactly the same way as the letter tokenizer with the only difference that it also lowercases letters.
   - Whitespace Tokenizer. Splits by whitespace and does not remove symbols.
   - UAX URL Email Tokenizer (uax_url_email). Like the snandard tokenizer, but treats URLs and email addresses as single tokens. 
      `"Contact us at info@company.com or visit https://company.com" -> [Contact, us, at, info@company.com, or, visit, https://company.com]`
      The standard tokenizer would split email addresses and urls into multiple tokens.
* Partial word tokenizers. Breaks up text or words into small fragments. Used for partial word matching. 
   - N-Gram Tokenizer (ngram). Breaks text into words when encountering certain characters and the emits N-grams of the specified length.
   `"Red wine" -> [Re, Red, ed, wi, win, wine, in, ine, ne]`
   It emits so-called n-grams of a specified length. **N-gram** is like a sliding window that moves across a word and includes a number of characters. You can think of it as a substring method, used in programing languages with an increasing start index. The number of characters included in an n-gram depends on how we configured it. In this exam we used min # of 2 and max # of 10. 
   The n-gram tokenizer splits the words by whitespace and then begins by generating n-grams for the first term. It takes 2 letters first, then it increases the # of characters until it either reaches the end of the term or reaches the max #. When it happens, it moves the cursor or start index position forward and starts generating n-grams from the position of the character that generates a new n-gram.
   - Edge N-Gram Tokenizer (edge_ngram). Breaks text into words when encountering certain characters and then emits N-grams of each word beginning from the start of the word.
   `"Red wine" -> [Re, Red, wi, win, wine]`. The start point of each n-gram would be 0 and only the length of the substring would increase. It might be used for autocompletion searches, but it is better to look at **suggestors**.
* Structured text tokenizers. Used for structured text such as e-mail addresses, zip codes, identifiers etc. 
   - Keyword Tokenizer (keyword). No-op tokenizer which outputs the exact same text as a single term.
     `"I'm in the mood for drinking semi-dry red wine" -> [I'm in the mood for drinking semi-dry red wine!]'`
     All it does is takes an input and returns as a single term. Since analyzers require one tokenizer to be specified this can be useful when you don't want to tokenize anything but instead want to do something with character filters or token filters.
   - Pattern Tokenizer (pattern). Uses a reges to split text into terms when matching a word separator. Alternatively capturers matched text as terms. It either matches token separators or use capturing groups and use the matched text as terms.
     `"I, like, red, wine!" -> [I, like, red, wine] (commas are not preserved, they are just separators.
   - Path Tokenizerr (path_hierarchy). Splits hierarchical valuse (e.g. file system paths) and emits a term for each component in the tree.
      `/path/to/some/directory -> [/path, /path/to, /path/to/some/, /path/to/some/directory]
